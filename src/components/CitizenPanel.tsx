/**
 * Citizen Panel Component
 * Live audio recorder with waveform, multilingual text input
 */

import React, { useState, useRef, useEffect } from 'react';
import { Button } from '@/components/ui/button';
import { Textarea } from '@/components/ui/textarea';
import { Select, SelectContent, SelectItem, SelectTrigger, SelectValue } from '@/components/ui/select';
import { Card } from '@/components/ui/card';
import { Badge } from '@/components/ui/badge';
import { Mic, MicOff, Square, Send, Volume2 } from 'lucide-react';
import { SUPPORTED_LANGUAGES } from '../services/bhashiniClient';

interface CitizenPanelProps {
  onSubmit: (data: { text?: string; audioBase64?: string; language: string }) => void;
  isProcessing: boolean;
}

interface RecordingState {
  isRecording: boolean;
  isPaused: boolean;
  duration: number;
  audioBlob: Blob | null;
  audioBase64: string | null;
}

export const CitizenPanel: React.FC<CitizenPanelProps> = ({ onSubmit, isProcessing }) => {
  const [selectedLanguageForText, setSelectedLanguageForText] = useState('hi'); // For text input only
  const [detectedLanguage, setDetectedLanguage] = useState<string | null>(null); // For detected audio language
  const [textInput, setTextInput] = useState('');
  const [recording, setRecording] = useState<RecordingState>({
    isRecording: false,
    isPaused: false,
    duration: 0,
    audioBlob: null,
    audioBase64: null
  });
  const [waveformData, setWaveformData] = useState<number[]>(new Array(20).fill(0));

  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const audioContextRef = useRef<AudioContext | null>(null);
  const analyserRef = useRef<AnalyserNode | null>(null);
  const streamRef = useRef<MediaStream | null>(null);
  const intervalRef = useRef<NodeJS.Timeout | null>(null);
  const waveformIntervalRef = useRef<NodeJS.Timeout | null>(null);

  // Cleanup on unmount
  useEffect(() => {
    return () => {
      stopRecording();
      if (streamRef.current) {
        streamRef.current.getTracks().forEach(track => track.stop());
      }
      if (audioContextRef.current) {
        audioContextRef.current.close();
      }
    };
  }, []);

  // Update waveform visualization
  const updateWaveform = () => {
    if (!analyserRef.current) return;

    const bufferLength = analyserRef.current.frequencyBinCount;
    const dataArray = new Uint8Array(bufferLength);
    analyserRef.current.getByteFrequencyData(dataArray);

    // Convert to normalized values for visualization
    const newWaveformData = [];
    const step = Math.floor(bufferLength / 20);
    
    for (let i = 0; i < 20; i++) {
      const start = i * step;
      const end = start + step;
      let sum = 0;
      
      for (let j = start; j < end; j++) {
        sum += dataArray[j];
      }
      
      const average = sum / step;
      newWaveformData.push(Math.min(average / 255, 1));
    }

    setWaveformData(newWaveformData);
  };

  const startRecording = async () => {
    console.log('Start recording button clicked');
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ 
        audio: {
          sampleRate: 16000,
          channelCount: 1,
          echoCancellation: true,
          noiseSuppression: true
        } 
      });
      
      console.log('Media stream obtained successfully');
      streamRef.current = stream;

      // Set up audio analysis for waveform
      audioContextRef.current = new AudioContext({ sampleRate: 16000 });
      analyserRef.current = audioContextRef.current.createAnalyser();
      const source = audioContextRef.current.createMediaStreamSource(stream);
      
      analyserRef.current.fftSize = 2048;
      source.connect(analyserRef.current);

      // Set up media recorder
      mediaRecorderRef.current = new MediaRecorder(stream, {
        mimeType: 'audio/webm;codecs=opus'
      });

      const audioChunks: Blob[] = [];

      mediaRecorderRef.current.ondataavailable = (event) => {
        audioChunks.push(event.data);
      };

      mediaRecorderRef.current.onstop = async () => {
        console.log('Recording stopped, processing audio...');
        const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });
        const audioBase64 = await blobToBase64(audioBlob);
        
        console.log('Audio processed, base64 length:', audioBase64.length);
        
        setRecording(prev => ({
          ...prev,
          audioBlob,
          audioBase64,
          isRecording: false
        }));
      };

      mediaRecorderRef.current.start();
      console.log('MediaRecorder started');

      // Start duration timer
      intervalRef.current = setInterval(() => {
        setRecording(prev => ({
          ...prev,
          duration: prev.duration + 0.1
        }));
      }, 100);

      // Start waveform updates
      waveformIntervalRef.current = setInterval(updateWaveform, 100);

      setRecording(prev => ({
        ...prev,
        isRecording: true,
        duration: 0
      }));

    } catch (error) {
      console.error('Error starting recording:', error);
      alert('Unable to access microphone. Please check permissions.');
    }
  };

  const stopRecording = () => {
    console.log('Stop recording button clicked');
    
    if (mediaRecorderRef.current && mediaRecorderRef.current.state !== 'inactive') {
      console.log('Stopping media recorder');
      mediaRecorderRef.current.stop();
    }

    if (streamRef.current) {
      console.log('Stopping media stream');
      streamRef.current.getTracks().forEach(track => track.stop());
    }

    if (intervalRef.current) {
      clearInterval(intervalRef.current);
      intervalRef.current = null;
    }

    if (waveformIntervalRef.current) {
      clearInterval(waveformIntervalRef.current);
      waveformIntervalRef.current = null;
    }

    setWaveformData(new Array(20).fill(0));
    console.log('Recording cleanup completed');
  };

  const blobToBase64 = (blob: Blob): Promise<string> => {
    return new Promise((resolve, reject) => {
      const reader = new FileReader();
      reader.onload = () => {
        const result = reader.result as string;
        const base64 = result.split(',')[1]; // Remove data:audio/webm;base64,
        resolve(base64);
      };
      reader.onerror = reject;
      reader.readAsDataURL(blob);
    });
  };

  const handleSubmit = async () => {
    console.log('Submit button clicked');
    console.log('canSubmit:', canSubmit);
    console.log('textInput:', textInput);
    console.log('recording.audioBase64:', !!recording.audioBase64);
    console.log('isProcessing:', isProcessing);
    
    if (!canSubmit) {
      console.log('Submit blocked by canSubmit condition');
      return;
    }

    try {
      let audio_url = '';
      
      // If audio is recorded, use the base64 audio data
      if (recording.audioBase64) {
        audio_url = `data:audio/webm;base64,${recording.audioBase64}`;
        console.log('Using audio data for submission');
      } else if (textInput.trim()) {
        console.log('Using text data for submission:', textInput.trim());
      }

      console.log('Making API call...');
      
      // Make the API call
      const response = await fetch('https://eiouekjxflstcaicmixv.supabase.co/functions/v1/translator', {
        method: 'POST',
        headers: {
          'Authorization': 'Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6ImVpb3Vla2p4ZmxzdGNhaWNtaXh2Iiwicm9sZSI6ImFub24iLCJpYXQiOjE3NTM4NjkwNTQsImV4cCI6MjA2OTQ0NTA1NH0.S1wfBYOoZmrHZTgryglciiPRiZU3CXSEjnGRRTfTgz8',
          'Content-Type': 'application/json'
        },
        body: JSON.stringify({
          audio_url: audio_url,
          source_lang: 'hi',
          target_lang: 'en',
          conversation_id: '4cfb1cd9-cb66-4550-ba3e-397806ef0543'
        })
      });

      console.log('API response status:', response.status);

      if (response.ok) {
        const result = await response.json();
        console.log('API call successful:', result);
        
        // Reset form after successful submission
        setTextInput('');
        setDetectedLanguage(null);
        setRecording({
          isRecording: false,
          isPaused: false,
          duration: 0,
          audioBlob: null,
          audioBase64: null
        });
      } else {
        console.error('API call failed:', response.status, response.statusText);
      }
    } catch (error) {
      console.error('Error making API call:', error);
    }
  };

  const canSubmit = (textInput.trim() || recording.audioBase64) && !isProcessing;

  return (
    <div className="h-full flex flex-col bg-card border-r border-border">
      {/* Header */}
      <div className="p-4 border-b border-border bg-saffron/5">
        <h2 className="text-lg font-pt-sans font-semibold text-foreground mb-2">
          ‡§®‡§æ‡§ó‡§∞‡§ø‡§ï ‡§™‡•à‡§®‡§≤ ‚Ä¢ Citizen Panel
        </h2>
        <div className="space-y-2">
          <div className="text-sm text-muted-foreground">
            üé§ Voice: Auto-detect language ‚Ä¢ üñäÔ∏è Text: Select language
          </div>
          <Select value={selectedLanguageForText} onValueChange={setSelectedLanguageForText}>
            <SelectTrigger className="focus-ring">
              <SelectValue placeholder="‡§≠‡§æ‡§∑‡§æ ‡§ö‡•Å‡§®‡•á‡§Ç ‚Ä¢ Select Language for Text" />
            </SelectTrigger>
            <SelectContent>
              {Object.entries(SUPPORTED_LANGUAGES).map(([code, name]: [string, string]) => (
                <SelectItem key={code} value={code}>
                  {name}
                </SelectItem>
              ))}
            </SelectContent>
          </Select>
          {detectedLanguage && (
            <Badge variant="outline" className="text-xs">
              üé§ Detected: {SUPPORTED_LANGUAGES[detectedLanguage]}
            </Badge>
          )}
        </div>
      </div>

      {/* Audio Recording Section */}
      <div className="p-4 border-b border-border">
        <Card className="p-4 bg-gradient-to-br from-saffron/10 to-green/10 border-saffron/20">
          <div className="flex flex-col items-center space-y-4">
            
            {/* Waveform Visualization */}
            <div className="flex items-end justify-center space-x-1 h-16 w-full">
              {waveformData.map((amplitude, index) => (
                <div
                  key={index}
                  className={`waveform-bar w-2 bg-primary transition-all duration-100 ${
                    recording.isRecording ? '' : 'opacity-30'
                  }`}
                  style={{
                    height: `${Math.max(4, amplitude * 60)}px`,
                    animationDelay: `${index * 0.1}s`
                  }}
                />
              ))}
            </div>

            {/* Recording Controls */}
            <div className="flex items-center space-x-4">
              <Button
                variant={recording.isRecording ? "destructive" : "default"}
                size="lg"
                onClick={recording.isRecording ? stopRecording : startRecording}
                disabled={isProcessing}
                className="btn-ripple focus-ring w-16 h-16 rounded-full"
                aria-label={recording.isRecording ? "Stop recording" : "Start recording"}
              >
                {recording.isRecording ? (
                  <Square className="w-6 h-6" />
                ) : (
                  <Mic className="w-6 h-6" />
                )}
              </Button>

              {recording.duration > 0 && (
                <Badge variant="secondary" className="px-3 py-1 text-sm">
                  {recording.duration.toFixed(1)}s
                </Badge>
              )}
            </div>

            {recording.audioBase64 && (
              <div className="flex items-center space-x-2 text-sm text-muted-foreground">
                <Volume2 className="w-4 h-4" />
                <span>Audio recorded ‚Ä¢ Ready to submit</span>
              </div>
            )}
          </div>
        </Card>
      </div>

      {/* Text Input Section */}
      <div className="flex-1 p-4 flex flex-col">
        <Textarea
          value={textInput}
          onChange={(e) => setTextInput(e.target.value)}
          placeholder={`‡§Ø‡§π‡§æ‡§Å ‡§ü‡§æ‡§á‡§™ ‡§ï‡§∞‡•á‡§Ç... ‚Ä¢ Type here in ${SUPPORTED_LANGUAGES[selectedLanguageForText]}...`}
          className="flex-1 resize-none focus-ring text-base"
          disabled={isProcessing || recording.isRecording}
          aria-label="Text input for your message"
        />
      </div>

      {/* Submit Section */}
      <div className="p-4 border-t border-border bg-green/5">
        <Button
          onClick={handleSubmit}
          disabled={!canSubmit}
          size="lg"
          className="w-full btn-ripple focus-ring bg-green hover:bg-green-dark text-white shadow-green"
          aria-label="Submit your message"
        >
          <Send className="w-5 h-5 mr-2" />
          {isProcessing ? '‡§™‡•ç‡§∞‡§∏‡§Ç‡§∏‡•ç‡§ï‡§∞‡§£... ‚Ä¢ Processing...' : '‡§≠‡•á‡§ú‡•á‡§Ç ‚Ä¢ Submit'}
        </Button>
      </div>
    </div>
  );
};